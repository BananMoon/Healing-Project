## 9-3. 웹 크롤러. 
웹에 있는 데이터를 긁어오는 크롤러(crawler)를 설계할 때 무한 루프에 빠지는 일을 방지하려면?

### 1. 언제 발생하는가?
웹을 클릭하면 다른 웹으로 넘어가는데 이러한 웹을 링크에 의해 만들어지는 그래프라고 보면, 사이클이 존재할 것이다.

사이클을 탐지하려면 해시테이블을 두어서 링크에 대한 방문 정보를 저장한다. (링크를 key로 넣고 방문 여부(boolean)를 value로 넣는다.)

### 2. '페이지를 방문한다'의 의미?
페이지는 url로 구분할 수 있다.
http://github.com/BananMoon?id=Spring_Study
http://github.com/BananMoon?id=Java_Study
두 url은 웹 애플리케이션이 id라는 유효한 값을 인식하면 다른 페이지가 나타나지만
인식하지 않는 인자는 같은 페이지(http://github.com/BananMoon)를 보여주게 된다.

그렇다고 페이지를 내용으로 구분하기에도 무리가 있다. (새 팝업창이 떴다해서 다른 페이지로 구분하는 것은 아니기 때문)

### 3. 해결; 페이지 간 유사성을 가늠해 본다.
페이지의 내용과 URL 을 토대로 검토하고, 페이지가 다른 페이지들과 충분히 비슷하다고 판단하면,
그 페이지에 연결된 페이지를 탐색 우선순위를 낮춘다.

### 4. 동작 방법
1. 크롤러가 탐색해야하는 모든 항목들을 DB에 저장한다.
2. 페이지를 열어서 해당 페이지의 특정 섹션과 URL을 토대로 시그니처(signature)를 생성한다.
3. DB 쿼리를 통해 해당 시그니처 페이지가 최근에 탐색된 적 있는지 탐색한다.
    - 없다면, 해당 페이지 탐색 후 그 페이지에 연결된 링크들을 db에 추가한다.
    - 있다면, 해당 페이지의 우선순위를 낮춰서 db에 추가한다. 

=> 어떻게 무한루프에 빠지지 않지?
웹 탐색 행위를 언젠가 완전히 끝내고 싶다면, 탐색하기 위한 최소의 우선순위 값을 설정해두면 된다.
반복 방문할 수록 최소 우선순위보다 낮아지게 되고, 탐색이 종료된다.
